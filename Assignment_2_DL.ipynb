{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense,Dropout\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "sGMYG1x6jlvW"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "t-uxFpHphIab"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load data (with Devanagari → Latin → freq columns)\n",
        "def load_data(file_path, num_samples=None):\n",
        "    input_texts = []\n",
        "    target_texts = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if num_samples and i >= num_samples:\n",
        "                break\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) >= 2:\n",
        "                latin_input = parts[1]  # Latin script is column 1\n",
        "                devanagari_output = '\\t' + parts[0] + '\\n'  # Column 0; add start/end tokens\n",
        "                input_texts.append(latin_input)\n",
        "                target_texts.append(devanagari_output)\n",
        "    return input_texts, target_texts\n",
        "\n",
        "# Tokenize character-wise\n",
        "def tokenize_char(sequences):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True, filters='')\n",
        "    tokenizer.fit_on_texts(sequences)\n",
        "    sequences_tensor = tokenizer.texts_to_sequences(sequences)\n",
        "    return sequences_tensor, tokenizer\n",
        "\n",
        "# Pad sequences\n",
        "def pad_sequences(sequences, maxlen=None):\n",
        "    return tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen, padding='post')\n",
        "\n",
        "# Full pipeline\n",
        "def preprocess_dataset(file_path, num_samples=None):\n",
        "    input_texts, target_texts = load_data(file_path, num_samples)\n",
        "\n",
        "    input_tensor_raw, inp_tokenizer = tokenize_char(input_texts)\n",
        "    target_tensor_raw, targ_tokenizer = tokenize_char(target_texts)\n",
        "\n",
        "    max_input_len = max(len(seq) for seq in input_tensor_raw)\n",
        "    max_target_len = max(len(seq) for seq in target_tensor_raw)\n",
        "\n",
        "    input_tensor = pad_sequences(input_tensor_raw, max_input_len)\n",
        "    target_tensor = pad_sequences(target_tensor_raw, max_target_len)\n",
        "\n",
        "    return (input_tensor, target_tensor,\n",
        "            inp_tokenizer, targ_tokenizer,\n",
        "            max_input_len, max_target_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/hi.translit.sampled.train.tsv'\n",
        "\n",
        "input_tensor, target_tensor, inp_tokenizer, targ_tokenizer, max_in_len, max_tar_len = preprocess_dataset(\n",
        "    file_path, num_samples=10000)\n",
        "\n",
        "print(f\"Input shape: {input_tensor.shape}\")\n",
        "print(f\"Target shape: {target_tensor.shape}\")\n",
        "print(f\"Input vocab size: {len(inp_tokenizer.word_index) + 1}\")\n",
        "print(f\"Target vocab size: {len(targ_tokenizer.word_index) + 1}\")\n",
        "\n",
        "# View a sample\n",
        "# idx = 0\n",
        "# decoded_input = ''.join(inp_tokenizer.index_word.get(i, '') for i in input_tensor[idx] if i != 0)\n",
        "# decoded_target = ''.join(targ_tokenizer.index_word.get(i, '') for i in target_tensor[idx] if i != 0)\n",
        "\n",
        "# print(f\"Latin Input     : {decoded_input}\")\n",
        "# print(f\"Devanagari Target: {decoded_target}\")\n",
        "\n",
        "for idx in range(10):  # Change range to view more or fewer samples\n",
        "    decoded_input = ''.join(inp_tokenizer.index_word.get(i, '') for i in input_tensor[idx] if i != 0)\n",
        "    decoded_target = ''.join(targ_tokenizer.index_word.get(i, '') for i in target_tensor[idx] if i != 0)\n",
        "\n",
        "    print(f\"Latin Input     : {decoded_input}\")\n",
        "    print(f\"Devanagari Target: {decoded_target}\")\n",
        "    print('-' * 40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNi7uR7ThRvd",
        "outputId": "47009edd-84d8-43d3-f7c2-5fd8ebd593b6"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (10000, 18)\n",
            "Target shape: (10000, 20)\n",
            "Input vocab size: 27\n",
            "Target vocab size: 64\n",
            "Latin Input     : an\n",
            "Devanagari Target: \tअं\n",
            "\n",
            "----------------------------------------\n",
            "Latin Input     : ankganit\n",
            "Devanagari Target: \tअंकगणित\n",
            "\n",
            "----------------------------------------\n",
            "Latin Input     : uncle\n",
            "Devanagari Target: \tअंकल\n",
            "\n",
            "----------------------------------------\n",
            "Latin Input     : ankur\n",
            "Devanagari Target: \tअंकुर\n",
            "\n",
            "----------------------------------------\n",
            "Latin Input     : ankuran\n",
            "Devanagari Target: \tअंकुरण\n",
            "\n",
            "----------------------------------------\n",
            "Latin Input     : ankurit\n",
            "Devanagari Target: \tअंकुरित\n",
            "\n",
            "----------------------------------------\n",
            "Latin Input     : aankush\n",
            "Devanagari Target: \tअंकुश\n",
            "\n",
            "----------------------------------------\n",
            "Latin Input     : ankush\n",
            "Devanagari Target: \tअंकुश\n",
            "\n",
            "----------------------------------------\n",
            "Latin Input     : ang\n",
            "Devanagari Target: \tअंग\n",
            "\n",
            "----------------------------------------\n",
            "Latin Input     : anga\n",
            "Devanagari Target: \tअंग\n",
            "\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# File paths for train, validation, and test datasets\n",
        "train_file = \"/content/hi.translit.sampled.train.tsv\"\n",
        "val_file = \"/content/hi.translit.sampled.dev.tsv\"\n",
        "test_file = \"/content/hi.translit.sampled.test.tsv\"\n",
        "\n",
        "# Load and preprocess training, validation, and test data\n",
        "train_input, train_target, inp_tokenizer, targ_tokenizer, _, _ = preprocess_dataset(train_file, num_samples=10000)\n",
        "val_input, val_target, _, _, _, _ = preprocess_dataset(val_file, num_samples=1000)\n",
        "test_input, test_target, _, _, _, _ = preprocess_dataset(test_file, num_samples=1000)\n"
      ],
      "metadata": {
        "id": "_luxyhBDlccI"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_one_hot_data(input_tensor, target_tensor, num_encoder_tokens, num_decoder_tokens):\n",
        "    num_samples = len(input_tensor)\n",
        "    max_input_len = input_tensor.shape[1]\n",
        "    max_target_len = target_tensor.shape[1]\n",
        "\n",
        "    encoder_input_data = np.zeros((num_samples, max_input_len, num_encoder_tokens), dtype=\"float32\")\n",
        "    decoder_input_data = np.zeros((num_samples, max_target_len, num_decoder_tokens), dtype=\"float32\")\n",
        "    decoder_target_data = np.zeros((num_samples, max_target_len, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        for t in range(max_input_len):\n",
        "            encoder_input_data[i, t, input_tensor[i, t]] = 1.0\n",
        "        for t in range(max_target_len):\n",
        "            decoder_input_data[i, t, target_tensor[i, t]] = 1.0\n",
        "            if t > 0:\n",
        "                decoder_target_data[i, t - 1, target_tensor[i, t]] = 1.0\n",
        "\n",
        "    return encoder_input_data, decoder_input_data, decoder_target_data\n"
      ],
      "metadata": {
        "id": "UjX9Xepxhe8L"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create one-hot encoded data for training, validation, and test sets\n",
        "train_encoder, train_decoder_in, train_decoder_out = create_one_hot_data(\n",
        "    train_input, train_target, len(inp_tokenizer.word_index)+1, len(targ_tokenizer.word_index)+1\n",
        ")\n",
        "\n",
        "val_encoder, val_decoder_in, val_decoder_out = create_one_hot_data(\n",
        "    val_input, val_target, len(inp_tokenizer.word_index)+1, len(targ_tokenizer.word_index)+1\n",
        ")\n",
        "\n",
        "test_encoder_input, test_decoder_in, test_decoder_out = create_one_hot_data(\n",
        "    test_input, test_target, len(inp_tokenizer.word_index)+1, len(targ_tokenizer.word_index)+1\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "JNynm57Rjva5"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an input sequence and process it.\n",
        "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "encoder = keras.layers.LSTM(latent_dim, return_state=True,dropout=0.3)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.3)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "7t_VPTRtmv1N"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "model.fit(\n",
        "    [train_encoder, train_decoder_in], train_decoder_out,\n",
        "    batch_size=64,\n",
        "    epochs=100,\n",
        "    validation_split=0.2,\n",
        ")\n",
        "# Save model\n",
        "model.save(\"s2s_model.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QknL_qOscVMb",
        "outputId": "113e1c50-45a5-40c5-9117-118892ede4a8"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.6186 - loss: 1.8596 - val_accuracy: 0.6626 - val_loss: 1.3850\n",
            "Epoch 2/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6773 - loss: 1.2859 - val_accuracy: 0.6940 - val_loss: 1.2804\n",
            "Epoch 3/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6917 - loss: 1.1960 - val_accuracy: 0.6946 - val_loss: 1.2615\n",
            "Epoch 4/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6984 - loss: 1.1567 - val_accuracy: 0.7003 - val_loss: 1.2911\n",
            "Epoch 5/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7067 - loss: 1.1169 - val_accuracy: 0.7081 - val_loss: 1.2756\n",
            "Epoch 6/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7125 - loss: 1.0803 - val_accuracy: 0.7058 - val_loss: 1.2511\n",
            "Epoch 7/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7169 - loss: 1.0587 - val_accuracy: 0.7183 - val_loss: 1.2380\n",
            "Epoch 8/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7208 - loss: 1.0348 - val_accuracy: 0.7227 - val_loss: 1.2244\n",
            "Epoch 9/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7219 - loss: 1.0262 - val_accuracy: 0.7315 - val_loss: 1.2408\n",
            "Epoch 10/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7275 - loss: 1.0033 - val_accuracy: 0.7424 - val_loss: 1.1918\n",
            "Epoch 11/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7309 - loss: 0.9948 - val_accuracy: 0.7372 - val_loss: 1.2080\n",
            "Epoch 12/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7372 - loss: 0.9651 - val_accuracy: 0.7413 - val_loss: 1.1714\n",
            "Epoch 13/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.7399 - loss: 0.9516 - val_accuracy: 0.7362 - val_loss: 1.1999\n",
            "Epoch 14/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7424 - loss: 0.9412 - val_accuracy: 0.7352 - val_loss: 1.1998\n",
            "Epoch 15/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7451 - loss: 0.9261 - val_accuracy: 0.7387 - val_loss: 1.1841\n",
            "Epoch 16/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7465 - loss: 0.9195 - val_accuracy: 0.7382 - val_loss: 1.1856\n",
            "Epoch 17/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7479 - loss: 0.9107 - val_accuracy: 0.7478 - val_loss: 1.1708\n",
            "Epoch 18/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7537 - loss: 0.8902 - val_accuracy: 0.7431 - val_loss: 1.1595\n",
            "Epoch 19/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7564 - loss: 0.8800 - val_accuracy: 0.7439 - val_loss: 1.1708\n",
            "Epoch 20/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7621 - loss: 0.8609 - val_accuracy: 0.7456 - val_loss: 1.1609\n",
            "Epoch 21/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7641 - loss: 0.8530 - val_accuracy: 0.7474 - val_loss: 1.1506\n",
            "Epoch 22/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7684 - loss: 0.8375 - val_accuracy: 0.7478 - val_loss: 1.1475\n",
            "Epoch 23/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7705 - loss: 0.8257 - val_accuracy: 0.7519 - val_loss: 1.1807\n",
            "Epoch 24/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7708 - loss: 0.8209 - val_accuracy: 0.7536 - val_loss: 1.1720\n",
            "Epoch 25/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7757 - loss: 0.8061 - val_accuracy: 0.7516 - val_loss: 1.1640\n",
            "Epoch 26/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7812 - loss: 0.7816 - val_accuracy: 0.7569 - val_loss: 1.1105\n",
            "Epoch 27/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7819 - loss: 0.7796 - val_accuracy: 0.7540 - val_loss: 1.1225\n",
            "Epoch 28/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7856 - loss: 0.7675 - val_accuracy: 0.7555 - val_loss: 1.1298\n",
            "Epoch 29/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7863 - loss: 0.7609 - val_accuracy: 0.7605 - val_loss: 1.1059\n",
            "Epoch 30/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7917 - loss: 0.7373 - val_accuracy: 0.7617 - val_loss: 1.1057\n",
            "Epoch 31/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.7930 - loss: 0.7333 - val_accuracy: 0.7601 - val_loss: 1.1155\n",
            "Epoch 32/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7942 - loss: 0.7266 - val_accuracy: 0.7572 - val_loss: 1.1287\n",
            "Epoch 33/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7973 - loss: 0.7182 - val_accuracy: 0.7614 - val_loss: 1.0931\n",
            "Epoch 34/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7989 - loss: 0.7085 - val_accuracy: 0.7604 - val_loss: 1.1014\n",
            "Epoch 35/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8016 - loss: 0.7004 - val_accuracy: 0.7666 - val_loss: 1.1006\n",
            "Epoch 36/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8021 - loss: 0.6924 - val_accuracy: 0.7649 - val_loss: 1.1049\n",
            "Epoch 37/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8056 - loss: 0.6842 - val_accuracy: 0.7654 - val_loss: 1.0969\n",
            "Epoch 38/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8062 - loss: 0.6844 - val_accuracy: 0.7649 - val_loss: 1.0990\n",
            "Epoch 39/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8084 - loss: 0.6729 - val_accuracy: 0.7659 - val_loss: 1.1217\n",
            "Epoch 40/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8097 - loss: 0.6703 - val_accuracy: 0.7665 - val_loss: 1.1000\n",
            "Epoch 41/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8126 - loss: 0.6591 - val_accuracy: 0.7671 - val_loss: 1.1015\n",
            "Epoch 42/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8123 - loss: 0.6638 - val_accuracy: 0.7698 - val_loss: 1.0831\n",
            "Epoch 43/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8140 - loss: 0.6530 - val_accuracy: 0.7718 - val_loss: 1.1050\n",
            "Epoch 44/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8150 - loss: 0.6503 - val_accuracy: 0.7681 - val_loss: 1.0900\n",
            "Epoch 45/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8169 - loss: 0.6430 - val_accuracy: 0.7663 - val_loss: 1.1255\n",
            "Epoch 46/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8191 - loss: 0.6349 - val_accuracy: 0.7657 - val_loss: 1.1054\n",
            "Epoch 47/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8198 - loss: 0.6333 - val_accuracy: 0.7685 - val_loss: 1.1364\n",
            "Epoch 48/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8213 - loss: 0.6249 - val_accuracy: 0.7706 - val_loss: 1.1027\n",
            "Epoch 49/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8213 - loss: 0.6246 - val_accuracy: 0.7704 - val_loss: 1.1211\n",
            "Epoch 50/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8242 - loss: 0.6142 - val_accuracy: 0.7690 - val_loss: 1.1038\n",
            "Epoch 51/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8233 - loss: 0.6169 - val_accuracy: 0.7663 - val_loss: 1.1496\n",
            "Epoch 52/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8230 - loss: 0.6183 - val_accuracy: 0.7699 - val_loss: 1.1026\n",
            "Epoch 53/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8257 - loss: 0.6088 - val_accuracy: 0.7694 - val_loss: 1.1414\n",
            "Epoch 54/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8305 - loss: 0.5986 - val_accuracy: 0.7720 - val_loss: 1.1009\n",
            "Epoch 55/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8296 - loss: 0.5987 - val_accuracy: 0.7729 - val_loss: 1.1011\n",
            "Epoch 56/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8296 - loss: 0.5936 - val_accuracy: 0.7705 - val_loss: 1.1250\n",
            "Epoch 57/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8303 - loss: 0.5907 - val_accuracy: 0.7689 - val_loss: 1.1388\n",
            "Epoch 58/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8305 - loss: 0.5911 - val_accuracy: 0.7688 - val_loss: 1.1352\n",
            "Epoch 59/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8344 - loss: 0.5779 - val_accuracy: 0.7712 - val_loss: 1.1428\n",
            "Epoch 60/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8347 - loss: 0.5767 - val_accuracy: 0.7686 - val_loss: 1.1157\n",
            "Epoch 61/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8352 - loss: 0.5795 - val_accuracy: 0.7694 - val_loss: 1.1625\n",
            "Epoch 62/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8365 - loss: 0.5694 - val_accuracy: 0.7674 - val_loss: 1.1740\n",
            "Epoch 63/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8349 - loss: 0.5741 - val_accuracy: 0.7722 - val_loss: 1.1039\n",
            "Epoch 64/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8367 - loss: 0.5676 - val_accuracy: 0.7745 - val_loss: 1.1097\n",
            "Epoch 65/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8398 - loss: 0.5592 - val_accuracy: 0.7709 - val_loss: 1.1380\n",
            "Epoch 66/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8406 - loss: 0.5568 - val_accuracy: 0.7737 - val_loss: 1.1178\n",
            "Epoch 67/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8419 - loss: 0.5535 - val_accuracy: 0.7772 - val_loss: 1.1166\n",
            "Epoch 68/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8399 - loss: 0.5530 - val_accuracy: 0.7727 - val_loss: 1.1345\n",
            "Epoch 69/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8408 - loss: 0.5553 - val_accuracy: 0.7724 - val_loss: 1.1388\n",
            "Epoch 70/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8423 - loss: 0.5501 - val_accuracy: 0.7724 - val_loss: 1.1120\n",
            "Epoch 71/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8437 - loss: 0.5471 - val_accuracy: 0.7710 - val_loss: 1.1611\n",
            "Epoch 72/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8415 - loss: 0.5509 - val_accuracy: 0.7738 - val_loss: 1.1453\n",
            "Epoch 73/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8425 - loss: 0.5499 - val_accuracy: 0.7720 - val_loss: 1.1486\n",
            "Epoch 74/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8455 - loss: 0.5417 - val_accuracy: 0.7739 - val_loss: 1.1389\n",
            "Epoch 75/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8448 - loss: 0.5453 - val_accuracy: 0.7717 - val_loss: 1.1604\n",
            "Epoch 76/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8488 - loss: 0.5321 - val_accuracy: 0.7715 - val_loss: 1.1683\n",
            "Epoch 77/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8450 - loss: 0.5403 - val_accuracy: 0.7766 - val_loss: 1.1299\n",
            "Epoch 78/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8474 - loss: 0.5382 - val_accuracy: 0.7752 - val_loss: 1.1707\n",
            "Epoch 79/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8487 - loss: 0.5329 - val_accuracy: 0.7745 - val_loss: 1.1376\n",
            "Epoch 80/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8493 - loss: 0.5266 - val_accuracy: 0.7723 - val_loss: 1.1509\n",
            "Epoch 81/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.8520 - loss: 0.5218 - val_accuracy: 0.7752 - val_loss: 1.1316\n",
            "Epoch 82/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8502 - loss: 0.5241 - val_accuracy: 0.7732 - val_loss: 1.1315\n",
            "Epoch 83/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8513 - loss: 0.5202 - val_accuracy: 0.7730 - val_loss: 1.1754\n",
            "Epoch 84/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8531 - loss: 0.5159 - val_accuracy: 0.7738 - val_loss: 1.1442\n",
            "Epoch 85/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8526 - loss: 0.5184 - val_accuracy: 0.7732 - val_loss: 1.1425\n",
            "Epoch 86/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8521 - loss: 0.5179 - val_accuracy: 0.7720 - val_loss: 1.1704\n",
            "Epoch 87/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8533 - loss: 0.5144 - val_accuracy: 0.7731 - val_loss: 1.1564\n",
            "Epoch 88/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8554 - loss: 0.5064 - val_accuracy: 0.7721 - val_loss: 1.1693\n",
            "Epoch 89/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8545 - loss: 0.5068 - val_accuracy: 0.7717 - val_loss: 1.1702\n",
            "Epoch 90/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8559 - loss: 0.5075 - val_accuracy: 0.7761 - val_loss: 1.1506\n",
            "Epoch 91/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8558 - loss: 0.5037 - val_accuracy: 0.7736 - val_loss: 1.1277\n",
            "Epoch 92/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8557 - loss: 0.5036 - val_accuracy: 0.7744 - val_loss: 1.1592\n",
            "Epoch 93/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8563 - loss: 0.5019 - val_accuracy: 0.7741 - val_loss: 1.1715\n",
            "Epoch 94/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8569 - loss: 0.5001 - val_accuracy: 0.7743 - val_loss: 1.1393\n",
            "Epoch 95/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8588 - loss: 0.4928 - val_accuracy: 0.7745 - val_loss: 1.1712\n",
            "Epoch 96/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8572 - loss: 0.5024 - val_accuracy: 0.7732 - val_loss: 1.1608\n",
            "Epoch 97/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8611 - loss: 0.4905 - val_accuracy: 0.7737 - val_loss: 1.1415\n",
            "Epoch 98/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8605 - loss: 0.4916 - val_accuracy: 0.7718 - val_loss: 1.1848\n",
            "Epoch 99/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8599 - loss: 0.4905 - val_accuracy: 0.7740 - val_loss: 1.1670\n",
            "Epoch 100/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.8616 - loss: 0.4868 - val_accuracy: 0.7746 - val_loss: 1.1417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"s2s_model.keras\")\n",
        "\n",
        "encoder_inputs = model.input[0]  # input_1\n",
        "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_inputs = model.input[1]  # input_2\n",
        "decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_lstm = model.layers[3]\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_dense = model.layers[4]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = keras.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in inp_tokenizer.word_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in targ_tokenizer.word_index.items())\n",
        "\n",
        "target_token_index = targ_tokenizer.word_index\n",
        "max_decoder_seq_length = max_tar_len\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value, verbose=0\n",
        "        )\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        if sampled_token_index == 0:\n",
        "            sampled_char = ''  # Or any placeholder you prefer, like ' '\n",
        "        else:\n",
        "            sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "Qh2du9UZdhL6"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seq_index in range(5):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = train_encoder[seq_index : seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print(\"-\")\n",
        "    input_text = ''.join([reverse_input_char_index.get(idx, '') for idx in train_input[seq_index] if idx != 0])\n",
        "    print(\"Input sentence:\", input_text)\n",
        "    print(\"Decoded sentence:\", decoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Btb2Vq-LdlJq",
        "outputId": "9e304f01-9401-48e0-d456-21c50ffd2a5e"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "Input sentence: an\n",
            "Decoded sentence: एन\n",
            "\n",
            "-\n",
            "Input sentence: ankganit\n",
            "Decoded sentence: अंगग्ित\n",
            "\n",
            "-\n",
            "Input sentence: uncle\n",
            "Decoded sentence: एर्क\n",
            "\n",
            "-\n",
            "Input sentence: ankur\n",
            "Decoded sentence: अंकर\n",
            "\n",
            "-\n",
            "Input sentence: ankuran\n",
            "Decoded sentence: अनुदानों\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq_index in range(10):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = test_encoder_input[seq_index : seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print(\"-\")\n",
        "    input_text = ''.join([reverse_input_char_index.get(idx, '') for idx in test_input[seq_index] if idx != 0])\n",
        "    print(\"Input sentence:\", input_text)\n",
        "    print(\"Decoded sentence:\", decoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQus0flXgr_m",
        "outputId": "c6590681-c84e-44a0-ca79-f215483afce0"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "Input sentence: ank\n",
            "Decoded sentence: अन्\n",
            "\n",
            "-\n",
            "Input sentence: anka\n",
            "Decoded sentence: अंका\n",
            "\n",
            "-\n",
            "Input sentence: ankir\n",
            "Decoded sentence: अनिर्र\n",
            "\n",
            "-\n",
            "Input sentence: anaksn\n",
            "Decoded sentence: अन्नान\n",
            "\n",
            "-\n",
            "Input sentence: anktsn\n",
            "Decoded sentence: अंक्तें\n",
            "\n",
            "-\n",
            "Input sentence: anksn\n",
            "Decoded sentence: अंक्\n",
            "\n",
            "-\n",
            "Input sentence: anpksh\n",
            "Decoded sentence: अप्काष\n",
            "\n",
            "-\n",
            "Input sentence: anksh\n",
            "Decoded sentence: आकक्ष\n",
            "\n",
            "-\n",
            "Input sentence: anpaahak\n",
            "Decoded sentence: अन्पाहही\n",
            "\n",
            "-\n",
            "Input sentence: anpahak\n",
            "Decoded sentence: अन्याकक\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WwdhFjMKndid"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}